{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from joblib import Parallel, delayed\n",
    "import tqdm\n",
    "from scipy.optimize import curve_fit\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append('../scales_project/')\n",
    "from utils import utils\n",
    "from utils import scale_by_scale_optim\n",
    "from utils import scale_fitter_no_grid\n",
    "from utils import evaluate\n",
    "from utils import simulate_EPR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def individual_locations(x,col='loc'):\n",
    "    ll = list(OrderedDict.fromkeys(x[col].values))\n",
    "    return x[col].map(dict(zip(ll,range(len(ll)))))\n",
    "\n",
    "def fit_scales(data_u, useruuid):\n",
    "\n",
    "    \n",
    "    # Stop locations median #\n",
    "    # ------------------- #\n",
    "    data_u['lat'] = data_u.groupby('loc')['lat'].transform('median')\n",
    "    data_u['lon'] = data_u.groupby('loc')['lon'].transform('median')\n",
    "\n",
    "    #data_u = data_u[data_u['loc']!=data_u['loc'].shift()]\n",
    "    data_u = data_u.dropna(subset = ['lat','lon'])\n",
    "    #data_u['loc'] = data_u['loc'].map(dict(zip(set(data_u['loc']),range(len(data_u['loc'])))))\n",
    "\n",
    "    \n",
    "    \n",
    "    stop_coords = data_u.groupby('loc')[['lat','lon']].median().values\n",
    "    series = np.array(list(map(int, data_u['loc'])))\n",
    "\n",
    "\n",
    "    # Find scale solution #\n",
    "    # ------------------- #\n",
    "    \n",
    "    my_split = scale_by_scale_optim.ScalesOptim(series, \n",
    "                                                stop_coords,\n",
    "                                                min_dist = 1.2, \n",
    "                                                nprocs = 1,\n",
    "                                                bootstrap = True,\n",
    "                                                information_criterion = None,\n",
    "                                                siglvl=0.4, \n",
    "                                                verbose = False)  \n",
    "    final_series, final_scales, likelihoods, criterion_s, final_sizes, final_proba_dist, final_alphas = my_split.find_best_scale()\n",
    "    \n",
    "    # Find number of containers per scale #\n",
    "    # ------------------------------------ #\n",
    "    containers_per_scale = [len(set(np.array(final_series)[:,i])) for i in range(len(final_series[0]))]\n",
    "  \n",
    "    trip_labels = [np.nan]+utils.get_scale_labels(final_series)\n",
    "    \n",
    "    data_u['trip_scale'] = trip_labels\n",
    "    data_u['sizes'] = [list(final_sizes.values())]*len(data_u)\n",
    "    data_u['scales'] = [list(final_scales.values())]* len(data_u)\n",
    "    data_u['containers'] = [list(containers_per_scale)]* len(data_u)\n",
    "    data_u['max_d'] = my_split.max_d\n",
    "    data_u['final_series'] = final_series\n",
    "\n",
    "    return data_u\n",
    "\n",
    "def fit_scale_pandas(dataframe_user, min_days):\n",
    "    user_name = dataframe_user['useruuid'].iloc[0]\n",
    "    days = set((dataframe_user.start/(60*60*24)).astype(int))\n",
    "    if ((dataframe_user.start.max() - dataframe_user.start.min())>=(min_days*60*60*24)) and len(days)>min_days:\n",
    "        dataframe_user = fit_scales(dataframe_user,user_name)\n",
    "\n",
    "        #Find home\n",
    "        i = dataframe_user.groupby('loc')['lat'].transform('size').sort_values(ascending = False).index.values[0]\n",
    "        home = dataframe_user.loc[i][['lat','lon']].values\n",
    "        dataframe_user['home'] = [home]*len(dataframe_user)    \n",
    "        return dataframe_user[['useruuid','loc','start','end','lat','lon','trip_scale','scales','sizes','containers','home','max_d','final_series']]\n",
    "    else:\n",
    "        return pd.DataFrame([],columns = ['useruuid','loc','start','end','lat','lon','trip_scale','scales','sizes','containers','home','max_d','final_series'])    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DAYS = 250\n",
    "LEN_TRAINING = 300\n",
    "LEN_TEST = 50\n",
    "N_TRACES = 100\n",
    "N_JOBS = 20\n",
    "BINS_DR = np.logspace(1,7,40)\n",
    "N_USERS = 9000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE TRAIN AND TEST SSET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = os.listdir('/data/work/user/laura_data/scale_trips_0.5/')[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.concat([pd.read_parquet('/data/work/user/laura_data/scale_trips_0.5/{}/'.format(i)) for i in buckets])\n",
    "user_data['loc'] = user_data['final_series'].apply(lambda x:x[-1])\n",
    "user_data = user_data[user_data['loc']!=-1].copy()\n",
    "user_data = user_data[['useruuid','loc','lat','lon','start','end']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [11:10, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#MAKE SURE LOCATIONS GO FROM 0 to N\n",
    "user_data = user_data.sort_values(by = ['useruuid','start']).reset_index()\n",
    "user_data = user_data[user_data.groupby('useruuid')['loc'].shift()!=user_data['loc']]\n",
    "user_data['loc'] = (user_data.groupby('useruuid').apply(individual_locations)).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N months of training\n",
    "training_data = user_data[user_data['start']<user_data.groupby('useruuid')['start'].transform('min')+(LEN_TRAINING*24*60*60)].copy()\n",
    "#50 steps of testinguseruuid\n",
    "testing_data = (user_data[user_data['start']>=user_data.groupby('useruuid')['start'].transform('min')+(LEN_TRAINING*24*60*60)]).groupby('useruuid').head(LEN_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT CONTAINER MODEL TO TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time = False\n",
    "\n",
    "if first_time==True:\n",
    "    #Find scales\n",
    "    df_final = Parallel(n_jobs=25)(delayed(fit_scale_pandas)(group, MIN_DAYS) for key, group in tqdm.tqdm(training_data.groupby('useruuid')))\n",
    "    df_final = pd.concat([i for i in df_final if len(i)>0])\n",
    "    users_scales = df_final[['useruuid','scales','sizes','containers','home','max_d']].drop_duplicates(subset = ['useruuid'])\n",
    "    users_trips = df_final[['useruuid','start','end','lat','lon','trip_scale','final_series','loc']]\n",
    "    users_scales.to_pickle('./modelling/scales_global.pkl')\n",
    "    users_trips.to_pickle('./modelling/scale_trips_global.pkl')\n",
    "\n",
    "    \n",
    "else:\n",
    "    users_trips = pd.read_pickle('./modelling/scale_trips_global.pkl')\n",
    "    users_trips = users_trips[['useruuid','start','end','lat','lon','trip_scale','final_series','loc']].copy()\n",
    "    users_scales = pd.read_pickle('./modelling/scales_global.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15047"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users_trips['useruuid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140390"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data[\"useruuid\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15047"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users_trips['useruuid'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMULATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_scales(key, \n",
    "                         group, \n",
    "                         N_traces, \n",
    "                         length=50):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate traces using the scales model\n",
    "    key: user_id\n",
    "    group: output of the scales fit\n",
    "    N_traces : number of traces\n",
    "    length: number of displacemnts\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    data = group['final_series'].values\n",
    "    stop_coords = [tuple(i) for i in group.groupby('loc')[['lat','lon']].median().values]\n",
    "    simulations = pd.DataFrame(columns=['loc','coords','sim'])\n",
    "    for n in range(N_traces):\n",
    "        d = pd.DataFrame(evaluate.scales_generator(data,length,stop_coords),columns=['loc','coords'])\n",
    "        d['sim'] = n\n",
    "        simulations = pd.concat([simulations,d])\n",
    "    simulations['sim'] = simulations['sim'].astype(int)\n",
    "    simulations.to_pickle(\"./modelling/scales_synthetic_3/{}\".format(key))\n",
    "    return simulations\n",
    "\n",
    "def generate_data_epr(key,group, \n",
    "                      N_traces, \n",
    "                      length=50,\n",
    "                      rho=0.6,\n",
    "                      gamma=0.21,\n",
    "                      alpha=0.55):\n",
    "    \"\"\"\n",
    "    Generate traces using the epr model\n",
    "    key: user_id\n",
    "    group: output of the scales fit\n",
    "    N_traces : number of traces\n",
    "    length: number of displacemnts\n",
    "    rho, gamma, alpha: parameters of the model\n",
    "    \n",
    "    \"\"\"\n",
    "    if os.path.exists(\"./modelling/epr_synthetic_3/{}\".format(key)):\n",
    "        return 0\n",
    "    group = group.copy()\n",
    "    model = simulate_EPR.EPRModel(distance='haversine',p_rho=rho,p_gamma=gamma,p_alpha=alpha)\n",
    "    stop_coords = [tuple(i) for i in group.groupby('loc')[['lat','lon']].median().sort_index().values]\n",
    "\n",
    "    simulations = pd.DataFrame(columns=['loc','coords','sim'])\n",
    "    for n in range(N_traces):\n",
    "        model.reset()\n",
    "        model.train(group['loc'].values,list(stop_coords))\n",
    "        model.run_simulation(length)\n",
    "        d = pd.DataFrame(model.records,columns=['loc','coords'])\n",
    "        d['sim'] = n\n",
    "        simulations = pd.concat([simulations,d])\n",
    "\n",
    "    pd.DataFrame(simulations).to_pickle(\"./modelling/epr_synthetic_3/{}\".format(key))\n",
    "\n",
    "    return simulations\n",
    "\n",
    "def generate_data_m_epr(key, \n",
    "                        group, \n",
    "                        N_traces, \n",
    "                        length=50,\n",
    "                        rho=0.6,\n",
    "                        gamma=0.21,\n",
    "                        alpha=0.55, \n",
    "                        memory=30):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate traces using the m_epr model\n",
    "    key: user_id\n",
    "    group: output of the scales fit\n",
    "    N_traces : number of traces\n",
    "    length: number of displacemnts\n",
    "    rho, gamma, alpha: parameters of the model\n",
    "    \n",
    "    \"\"\" \n",
    "    if os.path.exists(\"./modelling/m_epr_synthetic_3/{}\".format(key)):\n",
    "        return 0\n",
    "    group = group.copy()\n",
    "    model = simulate_EPR.EPRModel_with_memory(distance='haversine',memory=memory,p_rho=rho,p_gamma=gamma,p_alpha=alpha)\n",
    "    stop_coords = [tuple(i) for i in group.groupby('loc')[['lat','lon']].median().sort_index().values]\n",
    "\n",
    "\n",
    "    simulations = pd.DataFrame(columns=['loc','coords','sim'])\n",
    "    for n in range(N_traces):\n",
    "        model.reset()\n",
    "        model.train(group['loc'].values,list(stop_coords))\n",
    "        model.run_simulation(length)\n",
    "        d = pd.DataFrame(model.records,columns=['loc','coords'])\n",
    "        d['sim'] = n\n",
    "        simulations = pd.concat([simulations,d])\n",
    "\n",
    "    pd.DataFrame(simulations).to_pickle(\"./modelling/m_epr_synthetic_3/{}\".format(key))\n",
    "\n",
    "    return simulations\n",
    "\n",
    "\n",
    "def generate_data_recency_epr(key, \n",
    "                              group, \n",
    "                              N_traces, \n",
    "                              length=50,\n",
    "                              rho=0.6,\n",
    "                              gamma=0.21,\n",
    "                              alpha=0.55):\n",
    "    \"\"\"\n",
    "    Generate traces using the m_epr model\n",
    "    key: user_id\n",
    "    group: output of the scales fit\n",
    "    N_traces : number of traces\n",
    "    length: number of displacemnts\n",
    "    rho, gamma, alpha: parameters of the model\n",
    "    \n",
    "    \"\"\" \n",
    "    if os.path.exists(\"./modelling/recency_epr_synthetic_3/{}\".format(key)):\n",
    "        return 0\n",
    "    group = group.copy()\n",
    "    model = simulate_EPR.RecencyEPRModel(distance='haversine',p_rho=rho,p_gamma=gamma,p_alpha=alpha)\n",
    "    \n",
    "    stop_coords = group.groupby('loc')[['lat','lon']].median().sort_index().apply(lambda x:tuple(x),axis =1).to_dict()\n",
    "\n",
    "    simulations = pd.DataFrame(columns=['loc','coords','sim'])\n",
    "    for n in range(N_traces):\n",
    "        model.reset()\n",
    "        model.train(group['loc'].values,stop_coords)\n",
    "        model.run_simulation(length)\n",
    "        d = pd.DataFrame(model.records,columns=['loc','coords'])\n",
    "        d['sim'] = n\n",
    "        simulations = pd.concat([simulations,d])\n",
    "\n",
    "    pd.DataFrame(simulations).to_pickle(\"./modelling/recency_epr_synthetic_3/{}\".format(key))\n",
    "\n",
    "\n",
    "\n",
    "    return simulations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTE AGGREGATED STATISTICS FOR REAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dr(trace_coordinates, \n",
    "               bins = np.logspace(2,7,40)):\n",
    "    \"\"\"\n",
    "    Compute probability of displacements\n",
    "    trace_coordinates: list of lat,lon\n",
    "    \"\"\"\n",
    "    starts = np.array([list(i) for i in trace_coordinates[1:]])\n",
    "    ends = np.array([list(i) for i in trace_coordinates[:-1]])\n",
    "    distances = utils.haversine(starts,ends)\n",
    "    return np.histogram(distances, bins = bins,density = True)[0]\n",
    "\n",
    "\n",
    "def compute_radius(trace_locations,trace_coordinates):\n",
    "    \"\"\"\n",
    "    Compute radius of gyration.\n",
    "    trace_locations: list of location labels\n",
    "    trace_coordinates: list o lat,lon\n",
    "    \"\"\"\n",
    "    home_location = sorted(Counter(trace_locations).items(),key = lambda x:x[1])[-1][0]    \n",
    "    index = np.where(trace_locations==home_location)[0][0]\n",
    "    trace_coordinates = np.array([list(i) for i in trace_coordinates])[index:]\n",
    "    center_of_mass = [np.median(trace_coordinates[:i],axis =0) for i in range(1,len(trace_coordinates)+1)]\n",
    "\n",
    "    \n",
    "    rg =  [np.abs(i) for i in utils.haversine(trace_coordinates, center_of_mass)]\n",
    "    rg = np.cumsum(rg) / np.arange(1,len(rg)+1)\n",
    "\n",
    "    return rg\n",
    "\n",
    "\n",
    "def compute_distr_locations(trace_locations, N_loc=100):\n",
    "    \"\"\"\n",
    "    Compute frequency rank.\n",
    "    trace_locations: list of location labels\n",
    "    N: top locations to consider\n",
    "    \"\"\"\n",
    "    x = trace_locations\n",
    "    xx = [i/len(x) for i in sorted(Counter(x).values(), reverse = True)]\n",
    "    N = N_loc - len(xx)\n",
    "    s = sum(xx[:N_loc])\n",
    "    xx = [i/s for i in xx[:N_loc]]+[0]*N\n",
    "    return xx\n",
    "\n",
    "def compute_entropies_iteration(trace_locations, \n",
    "                                return_median=True):\n",
    "    \"\"\"\n",
    "    Compute entropy.\n",
    "    trace_locations: list of location labels\n",
    "    N: top locations to consider\n",
    "    \"\"\"\n",
    "    if len(trace_locations)>=LEN_TEST:\n",
    "        temp_entropy = evaluate.est_entropy(trace_locations)\n",
    "        aa = []\n",
    "        for _ in range(1):\n",
    "            unc_entropy = evaluate.est_entropy(np.random.choice(trace_locations, len(trace_locations), replace = False))\n",
    "            aa.append((temp_entropy - unc_entropy))\n",
    "        if return_median:\n",
    "            return np.median(aa)\n",
    "        else:\n",
    "            return aa\n",
    "    else:\n",
    "        if return_median:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    \n",
    "\n",
    "def compute_everything(trace):\n",
    "    \"\"\"\n",
    "    Compute all aggregated statistics\n",
    "    \"\"\"\n",
    "    if len(trace)>1:\n",
    "        trace_locations = trace['loc'].values\n",
    "        trace_coordinates = trace['coords'].values\n",
    "        dr = compute_dr(trace_coordinates, bins = BINS_DR)\n",
    "        radius = compute_radius(trace_locations,trace_coordinates)\n",
    "        distr_locations = compute_distr_locations(trace_locations)\n",
    "        entropy = compute_entropies_iteration(trace_locations)\n",
    "        return dr,radius,distr_locations,entropy\n",
    "    else:\n",
    "         return np.array([]),np.array([]),np.array([]),np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = testing_data.groupby('useruuid').first()[testing_data.groupby('useruuid').size()==LEN_TEST].index.values\n",
    "users = list(set(users).intersection(set(users_trips.useruuid)))[:N_USERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPUTE PROPERTIES REAL DATA\n",
    "training_data_filtered = training_data[training_data.useruuid.isin(users)].copy()\n",
    "testing_data_filtered = testing_data[testing_data.useruuid.isin(users)].copy()\n",
    "\n",
    "testing_data_filtered['coords'] = testing_data_filtered[['lat','lon']].apply(lambda x:tuple(x),axis = 1)\n",
    "training_data_filtered['coords'] = training_data_filtered[['lat','lon']].apply(lambda x:tuple(x),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_filtered.to_pickle(\"./modelling/test_data_3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9000/9000 [30:18<00:00,  4.95it/s] \n",
      "100%|██████████| 9000/9000 [01:36<00:00, 93.59it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "###COMPUTE PROPERTIES OF TRAINING AND TESTING DATA\n",
    "properties_training = Parallel(n_jobs=N_JOBS)(delayed(compute_everything)(group) for key, group in \n",
    "                                          tqdm.tqdm(training_data_filtered.groupby(\"useruuid\")))\n",
    "properties_testing = Parallel(n_jobs=N_JOBS)(delayed(compute_everything)(group) for key, group in \n",
    "                                         tqdm.tqdm(testing_data_filtered.groupby(\"useruuid\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN  SIMULATIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIND PARAMETERS\n",
    "\n",
    "#FIND EXPONENT OF THE POWER LAW DISTRIBUTION OF DISPLACEMENTS\n",
    "e = BINS_DR\n",
    "test = np.nanmean([i[0] for n,i in enumerate(properties_training) if len(i[0])>0],axis = 0)\n",
    "a, b = np.polyfit(np.log(e[5:-1]),np.log(test[5:]),1)\n",
    "\n",
    "\n",
    "\n",
    "#FIND RHO E GAMMA\n",
    "\n",
    "def count_locations(trace):\n",
    "    locations = set()\n",
    "    wait_before_discovery= [1]\n",
    "    S = 0\n",
    "    for loc in trace:\n",
    "        if loc in locations:\n",
    "            wait_before_discovery[S]+=1\n",
    "        else:\n",
    "            locations.update({loc})\n",
    "            wait_before_discovery.append(1)\n",
    "            S+=1\n",
    "    return [1/i for i in wait_before_discovery]\n",
    "            \n",
    "test = training_data.groupby('useruuid')['loc'].apply(count_locations)\n",
    "N = LEN_TEST #time to consiser\n",
    "p_S = np.nanmean([list(i[:N])+[np.nan]*(N-len(i)) for i in test.values], axis = 0)\n",
    "(rho, gamma),pcov = curve_fit(lambda S, rho, gamma: rho*S**(-gamma), range(1,N+1),p_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5157308434588996 0.9532176965416148 0.17952985578134117\n"
     ]
    }
   ],
   "source": [
    "print(a,rho,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauale/.conda/envs/laura_test/lib/python3.8/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "np.save(\"./modelling/properties_testing.npy\",properties_testing, allow_pickle=True)\n",
    "np.save(\"./modelling/properties_training.npy\",properties_training, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./modelling/scales_synthetic_3/\")\n",
    "os.makedirs(\"./modelling/epr_synthetic_3/\")\n",
    "os.makedirs(\"./modelling/m_epr_synthetic_3/\")\n",
    "os.makedirs(\"./modelling/recency_epr_synthetic_3/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,rho,gamma = -1.5157308434588996, 0.9532176965416148, 0.17952985578134117\n",
    "users = os.listdir(\"./modelling/scales_synthetic_3/\")\n",
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 52/52 [00:02<00:00, 20.73it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "first_time=True\n",
    "filtered_data = users_trips[users_trips.useruuid.isin(users)].groupby(\"useruuid\")\n",
    "\n",
    "if first_time==True:\n",
    "\n",
    "    \n",
    "    #Simulate\n",
    "    #traces = Parallel(n_jobs=N_JOBS)(delayed(generate_data_scales)(key,group,N_TRACES) for key, group in tqdm.tqdm(filtered_data))\n",
    "    #traces2 = Parallel(n_jobs=N_JOBS)(delayed(generate_data_epr)(key,group,N_TRACES,rho=rho,gamma=gamma,alpha=-1-a) for key, group in tqdm.tqdm(filtered_data))\n",
    "    #traces4 = Parallel(n_jobs=N_JOBS)(delayed(generate_data_m_epr)(key,group, N_TRACES, rho=rho,gamma=gamma,alpha=-1-a) for key, group in tqdm.tqdm(filtered_data))\n",
    "    traces6 = Parallel(n_jobs=N_JOBS)(delayed(generate_data_recency_epr)(key,group, N_TRACES,rho=rho,gamma=gamma,alpha=-1-a) for key, group in tqdm.tqdm(filtered_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 0)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered_data = users_trips[users_trips.useruuid.isin(users)].groupby(\"useruuid\")\n",
    "\n",
    "#C = set([key for key,group in filtered_data])\n",
    "A = set(testing_data_filtered.useruuid.unique())\n",
    "B = set(os.listdir(\"./modelling/epr_synthetic_3/\"))\n",
    "len(A.difference(B)), len(B.difference(A)), len(C.difference(A)), len(C.difference(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregated stats for simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(folder_name, N = -1):\n",
    "    users = os.listdir(\"./modelling/{}/\".format(folder_name))  \n",
    "    trace = []\n",
    "    for user in users[:N]:\n",
    "        d = pd.read_pickle(\"./modelling/{}/{}\".format(folder_name, user))\n",
    "        d['useruuid'] = user\n",
    "        trace.append(d)\n",
    "    trace = pd.concat(trace)\n",
    "    return trace\n",
    "\n",
    "def compute_average_for_simulated_data(d):\n",
    "\n",
    "        \n",
    "    prop_sim = [compute_everything(group[:LEN_TEST]) for key,group in d.groupby(\"useruuid\")]\n",
    "\n",
    "    a = np.mean(np.array([i[0] for i in prop_sim]),axis = 0)\n",
    "    a_2 = np.std(np.array([i[0] for i in prop_sim]),axis = 0)\n",
    "\n",
    "\n",
    "    b = np.nanmedian(np.array([list(i[1])+[np.nan]*(LEN_TEST-len(i[1])) for i in prop_sim]),axis = 0)\n",
    "    b_2 = np.nanpercentile(np.array([list(i[1])+[np.nan]*(LEN_TEST-len(i[1])) for i in prop_sim]),25,axis = 0)\n",
    "    b_3 = np.nanpercentile(np.array([list(i[1])+[np.nan]*(LEN_TEST-len(i[1])) for i in prop_sim]),75,axis = 0)\n",
    "\n",
    "    c = np.mean(np.array([i[2] for i in prop_sim]),axis = 0)\n",
    "    c_2 = np.std(np.array([i[2] for i in prop_sim]),axis = 0)\n",
    "\n",
    "    d = np.array([i[3] for i in prop_sim])\n",
    "    prop_sim = a,b,c,d\n",
    "    err_sim = a_2, b_2, b_3, c_2\n",
    "        \n",
    "\n",
    "    return prop_sim, err_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [2:10:16<00:00, 78.17s/it]  \n",
      "100%|██████████| 100/100 [2:32:07<00:00, 91.28s/it]  \n",
      "100%|██████████| 100/100 [2:12:57<00:00, 79.78s/it]  \n",
      "100%|██████████| 100/100 [1:45:24<00:00, 63.24s/it]  \n"
     ]
    }
   ],
   "source": [
    "N_JOBS=5\n",
    "##SIMULATIONS\n",
    "props = []\n",
    "errs = []\n",
    "for folder in ['epr_synthetic_3','scales_synthetic_3','m_epr_synthetic_3','recency_epr_synthetic_3']:\n",
    "    d = read_data(folder)\n",
    "    prop, err =  zip(*Parallel(n_jobs=N_JOBS)(delayed(compute_average_for_simulated_data)(d[d.sim==sim].copy()) for sim in tqdm.tqdm(d.sim.unique())))\n",
    "    props.append(prop)\n",
    "    errs.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-92181e31a172>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  np.save(\"./modelling/properties_synthetic_data.npy\",np.array(props), allow_pickle=True)\n",
      "<ipython-input-8-92181e31a172>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  np.save(\"./modelling/errors_synthetic_data.npy\",np.array(errs), allow_pickle=True)\n"
     ]
    }
   ],
   "source": [
    "##HERE ARE THE PROPERTIES OF THE SIMULATED DATA (FOR FINAL FIGURE)\n",
    "np.save(\"./modelling/properties_synthetic_data.npy\",np.array(props), allow_pickle=True)\n",
    "np.save(\"./modelling/errors_synthetic_data.npy\",np.array(errs), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "###DISTRIBUTION OF DISPLACEMENTS\n",
    "def mean_dr_distr(group_user,bins = BINS_DR):\n",
    "    \"\"\"\n",
    "    Compute average distribution across simulations\n",
    "    \"\"\"\n",
    "    return np.nanmean([compute_dr(group2['coords'][:50].values,bins = bins) for key2, group2 in group_user.groupby('sim')],axis = 0)\n",
    "\n",
    "\n",
    "def compute_likelihood_dr(trace_coordinates,\n",
    "                          hist,\n",
    "                          edges= np.logspace(3,7,40),\n",
    "                          len_trace=50):\n",
    "    '''\n",
    "    Compute likelihood of the trace given the hist\n",
    "    \n",
    "    '''\n",
    "\n",
    "    if len(trace_coordinates)<2:\n",
    "        return []\n",
    "    \n",
    "    \n",
    "    ##COMPUTE DISTANCES\n",
    "    starts = np.array([list(i) for i in trace_coordinates[1:]])\n",
    "    ends = np.array([list(i) for i in trace_coordinates[:-1]])\n",
    "    distances = utils.haversine(starts,ends)\n",
    "   \n",
    "    #FIND PROBABILITY OF EACH DISTANCE UNDER THE COMPUTED HIST\n",
    "    digitized = []\n",
    "    n=0\n",
    "    for i in np.digitize([k for k in distances if k>edges[0] and k<edges[-1]], edges):\n",
    "        if  hist[i-1]>0:\n",
    "            digitized.append(hist[i-1]*(edges[i] - edges[i-1]))\n",
    "        elif hist[i-1]<=0:\n",
    "            digitized.append(0.0000000000001 )\n",
    "        n+=1\n",
    "    \n",
    "    #RETURN LIKELIHOOD\n",
    "    L1 = np.log(digitized)#np.sum(-np.log(digitized))/n*len_trace\n",
    "    return L1\n",
    "\n",
    "    \n",
    "###DISTRIBUTION OF LOCATIONS\n",
    "\n",
    "def compute_pdf_locs(x,bins):\n",
    "    \"\"\"\n",
    "    Compute distribution of location frequencies.\n",
    "    \"\"\"\n",
    "    counts = list(Counter(x).values())\n",
    "    relative_counts = [i/sum(counts) for i in counts]\n",
    "    return np.histogram(relative_counts, bins = bins, density = True)[0]\n",
    "    \n",
    "    \n",
    "def pdf_location(group_user, bins):\n",
    "    return np.mean([compute_pdf_locs(group2['loc'].values,bins) for key2, group2 in group_user.groupby('sim')],axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "def compute_likelihood_locations(trace_locs,\n",
    "                                 hist, \n",
    "                                 edges= np.logspace(3,7,40),\n",
    "                                 len_trace=50):\n",
    "    if len(trace_locs)<2:\n",
    "        return []\n",
    "    \n",
    "    \n",
    "    ##COMPUTE DISTANCES\n",
    "    counts = list(Counter(trace_locs).values())\n",
    "    relative_counts = [i/sum(counts) for i in counts]\n",
    "\n",
    "    #FIND PROBABILITY OF EACH DISTANCE UNDER THE COMPUTED HIST\n",
    "    digitized = []\n",
    "    n=0\n",
    "    for i in np.digitize([k for k in relative_counts if k>edges[0] and k<edges[-1]], edges):\n",
    "        if  hist[i-1]>0:\n",
    "            digitized.append(hist[i-1]*(edges[i] - edges[i-1])) #probability to be in that bin\n",
    "        elif hist[i-1]<=0:\n",
    "            digitized.append(0.0000000000001 )\n",
    "        n+=1\n",
    "    \n",
    "    #RETURN LIKELIHOOD\n",
    "    L1 = np.log(digitized)#np.sum(-np.log(digitized))/n*len_trace #here I just make sure that we have 50 displacements for each user\n",
    "    return np.sum(L1)\n",
    "\n",
    "\n",
    "\n",
    "###DISTRIBUTION OF ENTROPY DIFFERENCE\n",
    "\n",
    "def compute_entropy_hist(trace_locations,bins=np.linspace(-2,2,40)):\n",
    "    aa = compute_entropies_iteration(trace_locations, return_median=False)\n",
    "    hist,edges = np.histogram(aa,bins=bins,density=True)\n",
    "    return hist\n",
    "\n",
    "def pdf_entropy(group_user, bins=np.linspace(-2,2,40)):\n",
    "    return np.nanmean([compute_entropy_hist(group2['loc'].values,bins) for key2, group2 in group_user.groupby('sim')],axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "def compute_likelihood_entropy(trace_locs,\n",
    "                             hist, \n",
    "                             edges= np.linspace(-2,2,40),\n",
    "                             len_trace=50):\n",
    "    if len(trace_locs)<2:\n",
    "        return np.nan\n",
    "    \n",
    "    \n",
    "    ##COMPUTE ENTROPIES\n",
    "    entropies = compute_entropies_iteration(trace_locs, return_median=False)\n",
    "\n",
    "    #FIND PROBABILITY OF EACH DISTANCE UNDER THE COMPUTED HIST\n",
    "    digitized = []\n",
    "    n=0\n",
    "    for i in np.digitize([k for k in entropies if k>edges[0] and k<edges[-1]], edges):\n",
    "        if  hist[i-1]>0:\n",
    "            digitized.append(hist[i-1]*(edges[i] - edges[i-1]))\n",
    "        elif hist[i-1]<=0:\n",
    "            digitized.append(0.0000000000001 )\n",
    "        n+=1\n",
    "    \n",
    "    #RETURN LIKELIHOOD\n",
    "    L1 = np.log(digitized)#np.sum(-np.log(digitized))/n*len_trace\n",
    "    return L1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_filtered = pd.read_pickle(\"./modelling/test_data_3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dr_akaike(name,testing_data, bins = np.logspace(3,7,40)):\n",
    "    d = read_data(name,N=-1)\n",
    "    pdfs =  Parallel(n_jobs=-1,backend='loky')(delayed(mean_dr_distr)(group.copy(),bins) for key,group in tqdm.tqdm(d.groupby(\"useruuid\"),position=0))\n",
    "    Ls = Parallel(n_jobs=-1,backend='loky')(delayed(compute_likelihood_dr)(testing_data[testing_data.useruuid==(key)]['coords'].values, pdf,bins) for (key,group),pdf in tqdm.tqdm(zip(d.groupby(\"useruuid\"),pdfs),position=0))\n",
    "    return Ls\n",
    "\n",
    "\n",
    "def compute_location_akaike(name, testing_data, bins = np.logspace(-6,1,40)):\n",
    "    d = read_data(name,N=-1)\n",
    "    pdfs =  Parallel(n_jobs=-1,backend='loky')(delayed(pdf_location)(group.copy(), bins) for key,group in tqdm.tqdm((d.groupby(\"useruuid\")),position=0))\n",
    "    Ls = Parallel(n_jobs=-1,backend='loky')(delayed(compute_likelihood_locations)(testing_data[testing_data.useruuid==(key)]['loc'].values, pdf,bins) for (key,group),pdf in tqdm.tqdm(zip(d.groupby(\"useruuid\"),pdfs),position=0))\n",
    "\n",
    "    return Ls\n",
    "\n",
    "def compute_entropy_akaike(name,testing_data,  bins = np.linspace(-2,2,40)):\n",
    "    d = read_data(name,N=-1)\n",
    "    pdfs =  Parallel(n_jobs=-1,backend='loky')(delayed(pdf_entropy)(group.copy(), bins) for key,group in tqdm.tqdm(((d.groupby(\"useruuid\"))),position=0))\n",
    "    Ls = Parallel(n_jobs=-1,backend='loky')(delayed(compute_likelihood_entropy)(testing_data[testing_data.useruuid==(key)]['loc'].values, pdf, bins) for (key,group),pdf in tqdm.tqdm(zip(d.groupby(\"useruuid\"),pdfs),position=0))\n",
    "    return Ls\n",
    "\n",
    "\n",
    "def loglikelihood_ratio(loglikelihoods1, loglikelihoods2,\n",
    "        nested=False, normalized_ratio=False):\n",
    "    \"\"\"\n",
    "    Calculates a loglikelihood ratio and the p-value for testing which of two\n",
    "    probability distributions is more likely to have created a set of\n",
    "    observations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    loglikelihoods1 : list or array\n",
    "        The logarithms of the likelihoods of each observation, calculated from\n",
    "        a particular probability distribution.\n",
    "    loglikelihoods2 : list or array\n",
    "        The logarithms of the likelihoods of each observation, calculated from\n",
    "        a particular probability distribution.\n",
    "    nested : bool, optional\n",
    "        Whether one of the two probability distributions that generated the\n",
    "        likelihoods is a nested version of the other. False by default.\n",
    "    normalized_ratio : bool, optional\n",
    "        Whether to return the loglikelihood ratio, R, or the normalized\n",
    "        ratio R/sqrt(n*variance)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    R : float\n",
    "        The loglikelihood ratio of the two sets of likelihoods. If positive,\n",
    "        the first set of likelihoods is more likely (and so the probability\n",
    "        distribution that produced them is a better fit to the data). If\n",
    "        negative, the reverse is true.\n",
    "    p : float\n",
    "        The significance of the sign of R. If below a critical value\n",
    "        (typically .05) the sign of R is taken to be significant. If above the\n",
    "        critical value the sign of R is taken to be due to statistical\n",
    "        fluctuations.\n",
    "    \"\"\"\n",
    "    from numpy import sqrt\n",
    "    from scipy.special import erfc\n",
    "\n",
    "    n = float(len(loglikelihoods1))\n",
    "\n",
    "    if n==0:\n",
    "        R = 0\n",
    "        p = 1\n",
    "        return R, p\n",
    "    from numpy import asarray\n",
    "    loglikelihoods1 = asarray(loglikelihoods1)\n",
    "    loglikelihoods2 = asarray(loglikelihoods2)\n",
    "\n",
    "    #Clean for extreme values, if any\n",
    "    from numpy import inf, log\n",
    "    from sys import float_info\n",
    "    min_val = log(10**float_info.min_10_exp)\n",
    "    loglikelihoods1[loglikelihoods1==-inf] = min_val\n",
    "    loglikelihoods2[loglikelihoods2==-inf] = min_val\n",
    "\n",
    "    R = sum(loglikelihoods1-loglikelihoods2)\n",
    "\n",
    "    from numpy import mean\n",
    "    mean_diff = mean(loglikelihoods1)-mean(loglikelihoods2)\n",
    "    variance = sum(\n",
    "            ( (loglikelihoods1-loglikelihoods2) - mean_diff)**2\n",
    "            )/n\n",
    "\n",
    "    if nested:\n",
    "        from scipy.stats import chi2\n",
    "        p = 1 - chi2.cdf(abs(2*R), 1)\n",
    "    else:\n",
    "        p = erfc( abs(R) / sqrt(2*n*variance))\n",
    "\n",
    "    if normalized_ratio:\n",
    "        R = R/sqrt(n*variance)\n",
    "\n",
    "    return R, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [57:35, ?it/s]\n",
      "383it [31:39,  4.96s/it]\n",
      "100%|██████████| 8999/8999 [00:52<00:00, 170.66it/s]\n",
      "8999it [03:34, 41.95it/s]\n",
      "100%|██████████| 8999/8999 [00:52<00:00, 172.43it/s]\n",
      "8999it [03:33, 42.13it/s]\n",
      "100%|██████████| 8999/8999 [00:42<00:00, 213.19it/s]\n",
      "8999it [03:31, 42.46it/s]\n",
      "100%|██████████| 8999/8999 [00:37<00:00, 239.85it/s]\n",
      "8999it [03:31, 42.54it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ls_dr = [compute_dr_akaike(i, \n",
    "                                 testing_data_filtered, \n",
    "                                 bins = np.logspace(1,7,40)) \n",
    "                                 for i in \n",
    "                       ['scales_synthetic_3','epr_synthetic_3','m_epr_synthetic_3','recency_epr_synthetic_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8999/8999 [01:14<00:00, 120.08it/s]\n",
      "8999it [03:35, 41.82it/s]\n",
      "100%|██████████| 8999/8999 [01:05<00:00, 137.02it/s]\n",
      "8999it [03:36, 41.49it/s]\n",
      "100%|██████████| 8999/8999 [01:03<00:00, 141.05it/s]\n",
      "8999it [03:37, 41.36it/s]\n",
      "100%|██████████| 8999/8999 [00:54<00:00, 164.24it/s]\n",
      "8999it [03:38, 41.15it/s]\n"
     ]
    }
   ],
   "source": [
    "ls_entr = [compute_entropy_akaike(i, \n",
    "                                 testing_data_filtered, \n",
    "                                 bins = np.linspace(-2,2,40)) \n",
    "                                 for i in \n",
    "                       ['scales_synthetic_3','epr_synthetic_3','m_epr_synthetic_3','recency_epr_synthetic_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8999/8999 [00:34<00:00, 259.47it/s]\n",
      "8999it [03:30, 42.84it/s]\n",
      "100%|██████████| 8999/8999 [00:38<00:00, 235.41it/s]\n",
      "8999it [03:30, 42.66it/s]\n",
      "100%|██████████| 8999/8999 [00:28<00:00, 310.78it/s]\n",
      "8999it [03:29, 42.86it/s]\n",
      "100%|██████████| 8999/8999 [00:28<00:00, 314.06it/s]\n",
      "8999it [03:32, 42.32it/s]\n"
     ]
    }
   ],
   "source": [
    "ls_location = [compute_location_akaike(i, \n",
    "                                 testing_data_filtered, \n",
    "                                 bins = np.logspace(-6,2,40)) \n",
    "                                 for i in \n",
    "                       ['scales_synthetic_3','epr_synthetic_3','m_epr_synthetic_3','recency_epr_synthetic_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 nan\n",
      "81659.73839917098 0.0\n",
      "367763.97806551756 0.0\n",
      "253090.63495127918 0.0\n",
      "0.0 nan\n",
      "93296.63554165776 0.0\n",
      "87617.89005702062 0.0\n",
      "497676.6884752944 0.0\n",
      "0.0 nan\n",
      "44490.701259172965 3.0325751576552672e-258\n",
      "47140.92986665705 4.1700139467009516e-281\n",
      "108554.08105607357 0.0\n"
     ]
    }
   ],
   "source": [
    "def significance(p):\n",
    "    if p<0.001:\n",
    "        return \"***\"\n",
    "    elif p<0.01:\n",
    "        return \"**\"\n",
    "    elif p<0.05:\n",
    "        return \"*\"\n",
    "    else:\n",
    "        return \"\"\n",
    "names = ['scales','epr','m_epr','recency_epr']\n",
    "from collections import defaultdict\n",
    "ratios = defaultdict(dict)\n",
    "names_distr = ['$P(\\Delta r)$', \"$P(f_l)$\",\"$P(S_{unc} - S_{temp})$\"]\n",
    "\n",
    "for n1, L in enumerate([ls_dr,ls_location,ls_entr]):\n",
    "    if n1!=1:\n",
    "        lognorms = [i for i in np.concatenate(L[0]) if not np.isnan(i)]\n",
    "    else:\n",
    "        lognorms = L[0]\n",
    "    for n2,_ in enumerate(L):\n",
    "        if n1!=1:\n",
    "            other = [i for i in np.concatenate(L[n2]) if not np.isnan(i)]\n",
    "        else:\n",
    "            other = L[n2]\n",
    "        R,p = loglikelihood_ratio(lognorms,other)\n",
    "        print(R,p)\n",
    "        ratios[names_distr[n1]][names[n2]] = str(int(R))+significance(p)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
